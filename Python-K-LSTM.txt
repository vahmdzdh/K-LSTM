# Enable GPU acceleration (if available)
import tensorflow as tf
print("GPU Available:", len(tf.config.list_physical_devices('GPU')) > 0)

# Load necessary libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import (
    mean_squared_error, mean_absolute_error, r2_score,
    precision_score, recall_score, f1_score, roc_curve, auc, classification_report, confusion_matrix
)
from google.colab import files
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from scipy.stats import kurtosis
import matplotlib.pyplot as plt

# Upload files interactively
print("Upload training data:")
train_file = files.upload()
print("Upload test data:")
test_file = files.upload()
print("Upload test truth data:")
test_truth_file = files.upload()

# Load datasets from uploaded files
train_data = pd.read_csv(list(train_file.keys())[0], sep='\t', header=0)
test_data = pd.read_csv(list(test_file.keys())[0], sep='\t', header=0)
test_truth_data = pd.read_csv(list(test_truth_file.keys())[0], sep='\t', header=0)

# Feature engineering for training data
train_data['days_to_failure'] = train_data.groupby('vehicleId')['days'].transform('max') - train_data['days']
train_data['target'] = np.where(train_data['days_to_failure'] <= 30, 1, 0)

# Feature engineering for test data
test_truth_data = test_truth_data.set_index('vehicleId')
test_truth_data['maximum_day'] = test_data.groupby('vehicleId')['days'].max() + test_truth_data['RUL']
test_data['days_to_failure'] = test_truth_data.loc[test_data['vehicleId'], 'maximum_day'].values - test_data['days']
test_data['target'] = np.where(test_data['days_to_failure'] <= 30, 1, 0)

# Scale data using MinMaxScaler
scaler = MinMaxScaler()
sensor_columns_train = [col for col in train_data.columns if col.startswith('s')]
sensor_columns_test = [col for col in test_data.columns if col.startswith('s')]

train_data[sensor_columns_train] = scaler.fit_transform(train_data[sensor_columns_train])
test_data[sensor_columns_test] = scaler.transform(test_data[sensor_columns_test])

# Impute missing values
imputer = SimpleImputer(strategy='mean')
train_data[sensor_columns_train] = imputer.fit_transform(train_data[sensor_columns_train])
test_data[sensor_columns_test] = imputer.transform(test_data[sensor_columns_test])

# Prepare features and target variables
X_train = train_data.iloc[:, 2:-2].values  # Exclude non-feature columns
Y_train = train_data['target'].values
X_test = test_data.iloc[:, 2:-2].values    # Exclude non-feature columns
Y_test = test_data['target'].values

# Function to compute kurtosis for each sequence
def compute_kurtosis_for_sequences(data, sequence_length):
    """
    Compute a single kurtosis value for each sequence of length `sequence_length`.
    """
    num_samples = data.shape[0] // sequence_length
    kurtosis_values = []
    for i in range(num_samples):
        sequence = data[i * sequence_length:(i + 1) * sequence_length]
        if sequence.shape[0] == sequence_length:  # Ensure full sequences
            kurtosis_values.append(kurtosis(sequence.flatten(), fisher=True))  # Flatten and compute kurtosis
    return np.array(kurtosis_values)

# Compute kurtosis for each sequence
sequence_length = 30
train_kurtosis = compute_kurtosis_for_sequences(X_train, sequence_length)
test_kurtosis = compute_kurtosis_for_sequences(X_test, sequence_length)

# Reshape data for LSTM
def reshape_for_lstm(X, sequence_length=30):
    num_samples = X.shape[0] // sequence_length
    X_reshaped = X[:num_samples * sequence_length].reshape(num_samples, sequence_length, X.shape[1])
    return X_reshaped

X_train_reshaped = reshape_for_lstm(X_train, sequence_length)
X_test_reshaped = reshape_for_lstm(X_test, sequence_length)

# Reshape kurtosis values to (num_samples, 1, 1)
train_kurtosis_reshaped = train_kurtosis.reshape(-1, 1, 1)
test_kurtosis_reshaped = test_kurtosis.reshape(-1, 1, 1)

# Combine kurtosis with existing features
X_train_reshaped = np.concatenate([X_train_reshaped, np.repeat(train_kurtosis_reshaped, sequence_length, axis=1)], axis=-1)
X_test_reshaped = np.concatenate([X_test_reshaped, np.repeat(test_kurtosis_reshaped, sequence_length, axis=1)], axis=-1)

# Adjust target variables to match reshaped data
Y_train_reshaped = Y_train[:X_train_reshaped.shape[0] * sequence_length].reshape(-1, sequence_length)
Y_test_reshaped = Y_test[:X_test_reshaped.shape[0] * sequence_length].reshape(-1, sequence_length)

# Define the LSTM model
model = Sequential([
    LSTM(64, input_shape=(sequence_length, X_train_reshaped.shape[2]), return_sequences=True),
    Dropout(0.2),
    LSTM(32, return_sequences=False),
    Dropout(0.2),
    Dense(1, activation='sigmoid')  # Use 'sigmoid' for binary classification
])

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Train the model with early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
history = model.fit(
    X_train_reshaped, Y_train_reshaped[:, -1],  # Use the last timestep's target for training
    epochs=20,
    batch_size=32,
    validation_data=(X_test_reshaped, Y_test_reshaped[:, -1]),
    callbacks=[early_stopping],
    verbose=1
)

# Evaluate the model
y_pred = model.predict(X_test_reshaped)
y_pred_binary = np.where(y_pred >= 0.5, 1, 0)  # Convert probabilities to binary predictions

# Error Analysis
mse = mean_squared_error(Y_test_reshaped[:, -1], y_pred)
mae = mean_absolute_error(Y_test_reshaped[:, -1], y_pred)
r2 = r2_score(Y_test_reshaped[:, -1], y_pred)
precision = precision_score(Y_test_reshaped[:, -1], y_pred_binary)
recall = recall_score(Y_test_reshaped[:, -1], y_pred_binary)
f1 = f1_score(Y_test_reshaped[:, -1], y_pred_binary)

print("Mean Squared Error: ", mse)
print("Mean Absolute Error: ", mae)
print("R-squared: ", r2)
print("Precision: ", precision)
print("Recall: ", recall)
print("F1 Score: ", f1)

# Classification report and confusion matrix
print("\nClassification Report:")
print(classification_report(Y_test_reshaped[:, -1], y_pred_binary))

print("\nConfusion Matrix:")
print(confusion_matrix(Y_test_reshaped[:, -1], y_pred_binary))

# ROC Curve
fpr, tpr, thresholds = roc_curve(Y_test_reshaped[:, -1].flatten(), y_pred.flatten())
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()